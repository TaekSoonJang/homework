{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "from collections import namedtuple\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import tf_util, load_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [f.split('.')[0] for f in os.listdir('./experts')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.s = []\n",
    "        self.a = []\n",
    "        self.r = []\n",
    "    \n",
    "    def add(self, s, a, r):\n",
    "        self.s.append(s)\n",
    "        self.a.append(a)\n",
    "        self.r.append(r)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        for i in range(0, len(self.s), batch_size):\n",
    "            yield self.s[i:i+batch_size], self.a[i:i+batch_size], self.r[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertAgent:\n",
    "    def __init__(self, envname):\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            tf_util.initialize()\n",
    "            self.get_expert_policy = load_policy.load_policy('experts/{}.pkl'.format(envname))\n",
    "\n",
    "        self.sess = sess\n",
    "        self.env = gym.make(envname)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        with self.sess.as_default():\n",
    "            return self.get_expert_policy(state)\n",
    "    \n",
    "    def run_episode(self, max_iter=1):\n",
    "        maxstep = self.env.spec.max_episode_steps\n",
    "        \n",
    "        buf = ReplayBuffer()\n",
    "        for _ in range(max_iter):\n",
    "            s = self.env.reset()\n",
    "            for _ in range(maxstep):\n",
    "                a = self.get_action([s])[0]\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                buf.add(s, a, r)\n",
    "                s = s_\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "        return buf\n",
    "    \n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 envname,\n",
    "                 name='behavior_clone',\n",
    "                 learning_rate=0.001,\n",
    "                 logdir='tb_log'):\n",
    "        # Create environments\n",
    "        env = gym.make(envname)\n",
    "        s_size = env.observation_space.shape\n",
    "        a_size = env.action_space.shape\n",
    "        \n",
    "        # Create graph\n",
    "        graph = tf.Graph()\n",
    "\n",
    "        with graph.as_default():\n",
    "            s_pl = tf.placeholder(dtype=tf.float32, shape=(None, *s_size), name='state')\n",
    "            a_pl = tf.placeholder(dtype=tf.float32, shape=(None, *a_size), name='target_action')\n",
    "\n",
    "            with tf.variable_scope('my_policy'):\n",
    "                hidden1 = tf.layers.dense(s_pl, 200, activation=tf.nn.relu, name='hidden1')\n",
    "                hidden2 = tf.layers.dense(hidden1, 100, activation=tf.nn.relu, name='hidden2')\n",
    "\n",
    "                a_logit = tf.layers.dense(hidden2, a_size[0], activation=tf.tanh, name='logit')\n",
    "\n",
    "            with tf.variable_scope('training'):\n",
    "                loss = tf.losses.mean_squared_error(a_pl, a_logit, reduction=tf.losses.Reduction.MEAN)\n",
    "                train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "                tf.summary.scalar('loss/MSE', loss)\n",
    "\n",
    "            summary_merged = tf.summary.merge_all()\n",
    "        \n",
    "        # Initialize\n",
    "        sess = tf.Session(graph=graph)\n",
    "        \n",
    "        var_list = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        sess.run(tf.variables_initializer(var_list))\n",
    "\n",
    "        ts = datetime.datetime.now()\n",
    "        logpath = '{}/{}/{}'.format(logdir, name, ts)\n",
    "        writer = tf.summary.FileWriter(logpath, graph)\n",
    "        \n",
    "        # Tensorflow-related\n",
    "        self.graph = graph\n",
    "        self.sess = sess\n",
    "        self.s_pl = s_pl\n",
    "        self.a_pl = a_pl\n",
    "        self.a_logit = a_logit\n",
    "        self.train_op = train_op\n",
    "        self.writer = writer\n",
    "        self.summary_merged = summary_merged\n",
    "        \n",
    "        self.name = name\n",
    "        self.ts = ts\n",
    "        self.logpath = logpath\n",
    "        \n",
    "        self.env = env\n",
    "        self.envname = envname\n",
    "        \n",
    "        self.n_iter = 0\n",
    "        self.n_train = 0\n",
    "    \n",
    "    def get_action(self, s):\n",
    "        return self.sess.run(self.a_logit, feed_dict={self.s_pl: s})\n",
    "    \n",
    "    def run_episode(self, max_iter=1):\n",
    "        maxstep = self.env.spec.max_episode_steps\n",
    "        buf = ReplayBuffer()\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            s = self.env.reset()\n",
    "\n",
    "            for n_step in range(maxstep):\n",
    "                a = self.get_action([s])\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "\n",
    "                buf.add(s, a, r)\n",
    "\n",
    "                s = s_\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward', simple_value=sum(buf.r))])\n",
    "            self.writer.add_summary(reward_summary, self.n_iter)\n",
    "            self.n_iter += 1\n",
    "        \n",
    "        return buf\n",
    "    \n",
    "    def train(self, s_batch, a_target):\n",
    "        summary, _ = self.sess.run([self.summary_merged, self.train_op],\n",
    "                                   feed_dict={self.s_pl: s_batch, self.a_pl: a_target})\n",
    "        self.writer.add_summary(summary, self.n_train)\n",
    "        self.n_train += 1\n",
    "    \n",
    "    def tensorboard_scalar_url(self, tag):\n",
    "        tb_url = 'http://localhost:6006/data/plugin/scalars/scalars'\n",
    "        query = {\n",
    "            'run': '{}/{}'.format(self.name, self.ts),\n",
    "            'tag': tag,\n",
    "            'format': 'csv'\n",
    "        }\n",
    "        \n",
    "        return '{}?{}'.format(tb_url, urlencode(query))\n",
    "    \n",
    "    def plot_stats(self):\n",
    "        self.writer.flush()\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 4))\n",
    "        fig.suptitle(self.name, fontsize=16)\n",
    "        \n",
    "        reward_df = pd.read_csv(self.tensorboard_scalar_url('reward'))\n",
    "        loss_df = pd.read_csv(self.tensorboard_scalar_url('training/loss/MSE'))\n",
    "        \n",
    "        reward_ax = reward_df.plot(x='Step', y='Value', title='reward', ax=axes[0])\n",
    "        loss_ax = loss_df.plot(x='Step', y='Value', title='loss/MSE', ax=axes[1])\n",
    "        \n",
    "        return reward_ax, loss_ax\n",
    "    \n",
    "    def get_rewards(self):\n",
    "        return pd.read_csv(self.tensorboard_scalar_url('reward'), usecols=['Value']).Value.values\n",
    "    \n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "maxiter = 100\n",
    "n_rollouts = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_behavior_cloning_agent(envname):\n",
    "    agent = Agent(envname, name='behavior_clone_' + envname)\n",
    "    expert = ExpertAgent(envname)\n",
    "    \n",
    "    target_data = expert.run_episode(maxiter)\n",
    "    \n",
    "    for s_batch, a_batch, _ in target_data.get_batch(batch_size):\n",
    "        agent.train(s_batch, a_batch)\n",
    "    \n",
    "    return agent, expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_agents = [fit_behavior_cloning_agent(envname) for envname in envs]\n",
    "experts_rewards = []\n",
    "for a, e in bc_agents:\n",
    "    a.run_episode(n_rollouts)\n",
    "    exp_rewards = [sum(e.run_episode().r) for _ in range(n_rollouts)]\n",
    "    \n",
    "    reward_ax, loss_ax = a.plot_stats()\n",
    "    \n",
    "    experts_rewards.append(exp_rewards)\n",
    "    reward_ax.plot(np.arange(len(exp_rewards)), exp_rewards)\n",
    "    reward_ax.set_xlabel('iteration')\n",
    "    reward_ax.legend(['bc', 'expert'])\n",
    "    \n",
    "    loss_ax.legend(['bc'])\n",
    "    \n",
    "    a.close()\n",
    "    e.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { \n",
    "    'bc_mean': [agent.get_rewards().mean() for agent, _ in bc_agents],\n",
    "    'bc_std': [agent.get_rewards().std() for agent, _ in bc_agents],\n",
    "    'expert_mean': [np.array(exp_rewards).mean() for exp_rewards in experts_rewards],\n",
    "    'expert_std': [np.array(exp_rewards).std() for exp_rewards in experts_rewards],\n",
    "}\n",
    "\n",
    "indices = [agent.envname for agent, _ in bc_agents]\n",
    "\n",
    "df = pd.DataFrame(data, indices)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dagger_agent(envname):\n",
    "    agent = Agent(envname, name='dagger_' + envname)\n",
    "    expert = ExpertAgent(envname)\n",
    "    \n",
    "    for _ in range(maxiter):\n",
    "        episode_buf = agent.run_episode()\n",
    "\n",
    "        for s_batch, _, _ in episode_buf.get_batch(batch_size):\n",
    "            expert_action = expert.get_action(s_batch)\n",
    "            agent.train(s_batch, expert_action)\n",
    "    \n",
    "    agent.close()\n",
    "    \n",
    "    return agent, expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dagger_agents = [fit_dagger_agents(envname) for envname in envs]\n",
    "for a, e in dagger_agents:\n",
    "    exp_rewards = [sum(e.run_episode().r) for _ in range(maxiter)]\n",
    "    reward_ax, loss_ax = a.plot_stats()\n",
    "    \n",
    "    reward_ax.plot(np.arange(len(exp_rewards)), exp_rewards)\n",
    "    reward_ax.set_xlabel('iteration')\n",
    "    reward_ax.legend(['DAgger', 'expert'])\n",
    "    \n",
    "    loss_ax.legend(['DAgger'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
